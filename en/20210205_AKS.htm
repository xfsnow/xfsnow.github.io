<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Snowpeak Blog</title><meta name="description" content="Sharing and discussing cutting-edge technologies of AI and cloud computing, and other interesting topics."><meta name="keywords" content="AI, GitHub Copilot, Azure Cloud, Cloud Computing, Front-end Technology, Back-end Technology, Web Development, Software Engineering"><link rel="stylesheet" href="/assets/css/style.min.css"><link rel="stylesheet" href="https://site-assets.fontawesome.com/releases/v6.7.2/css/all.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script></head><body><nav class="navbar"><div class="nav-container"><div class="nav-logo"><h2><i class="fas fa-snowflake"></i> Snowpeak</h2></div><div class="nav-menu"><a href="/en/" class="nav-link">Home</a><a href="/en/page_1.htm" class="nav-link">Article</a><a href="/en#tools" class="nav-link">Tools</a><a href="/en/about.htm" class="nav-link">About</a><a href="/" class="nav-link lang-switch"><i class="fas fa-globe"></i> 简体中文</a></div><div class="nav-toggle" id="mobile-menu"><span class="bar"></span><span class="bar"></span><span class="bar"></span></div></div></nav><header class="article-header"><div class="container"><h1>Deploying Resilient Applications on Microsoft Azure AKS</h1><div class="article-meta"><span><i class="fas fa-calendar"></i> Published: 2021-02-05 16:25:06</span><span><i class="fas fa-clock"></i> Reading time: 25 minutes </span><span><i class="fas fa-tag"></i> Category: Tools</span></div></div></header><main class="article-container"><p>Azure Kubernetes Service (AKS) is a managed Kubernetes cluster on Microsoft Azure cloud that can be used to quickly deploy Kubernetes clusters. Combined with other Azure services and features, it simplifies daily operations and easily implements business application elasticity. This article is a hands-on experiment demonstrating the basic steps of elastic deployment. The applicable scenario is when virtual machines behind AKS experience unexpected shutdowns, automatic failover is achieved through Kubernetes configuration.</p><p>Reading this article requires mastering <a href="https://kubernetes.io/docs/">basic Kubernetes knowledge and operations</a>, as well as AKS <a href="https://docs.microsoft.com/en-us/azure/aks/">basic concepts</a> and <a href="https://docs.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster">deployment</a>.</p><h2>Basic Deployment</h2><h3>Application</h3><p>The demo application source code is here:</p><p><a href="https://github.com/xfsnow/pipelines-javascript-docker/">https://github.com/xfsnow/pipelines-javascript-docker/</a></p><p>Please fork it to your own GitHub account first.</p><p>This application is a very simple Node.js application that outputs time and hostname to demonstrate which backend it's running on after load balancing. There's only one main file:</p><p><a href="https://github.com/xfsnow/pipelines-javascript-docker/blob/a0aa063b97d6d6819c2adcdcea9b11e47959a86b/app/server.js#L19">https://github.com/xfsnow/pipelines-javascript-docker/blob/a0aa063b97d6d6819c2adcdcea9b11e47959a86b/app/server.js#L19</a></p><p>The output content is the following line:</p> <pre><code class="language-javascript">res.send('Hello world! Now is '+now+'.\nRunning on '+os.hostname()+'.');
</code></pre> <p>The main Kubernetes deployment file is manifests/hello-deployment.yml, deploying a deployment with 4 replicas. The declarations and functions of this configuration file are written in the relevant comments. The core principle is using:</p> <pre><code class="language-yaml"># Use toleration to detect node status, stop placing pods once nodes become unavailable
tolerations:
- key: &quot;node.kubernetes.io/unreachable&quot;
  operator: &quot;Exists&quot;
  effect: &quot;NoExecute&quot;
  tolerationSeconds: 1
- key: &quot;node.kubernetes.io/not-ready&quot;
  operator: &quot;Exists&quot;
  effect: &quot;NoExecute&quot;
  tolerationSeconds: 1
</code></pre> <p>And:</p> <pre><code class="language-yaml"># Health checks, check application running status through httpGet
livenessProbe:
  httpGet:
    port: 80
    scheme: HTTP
    path: /
  # Initial delay of 3 seconds for waiting time during first startup to avoid slow startup being misjudged as failure
  initialDelaySeconds: 10
  # Check interval after that
  periodSeconds: 1
  # How long timeout is considered failure
  timeoutSeconds: 3
</code></pre> <p>There's also a hello-service.yml that deploys a load balancer, exposing a public IP for external service. This load balancer also has health checks - when it detects backend pods are unavailable, it removes them and then mounts newly generated pods.</p><h3>Azure Resources</h3><p>An ACR for storing container images. Note that currently only the East 2 region supports the az acr build command to build and push directly to ACR locally. So we create the ACR resource in East 2, while the AKS cluster can be in any domestic region.</p> <pre><code class="language-bash"># Define environment variables
REGION_NAME=ChinaEast2
RESOURCE_GROUP=aksResilience
AKS_CLUSTER_NAME=aksResilience
ACR_NAME=acr$RANDOM

# Create resource group
az group create --location $REGION_NAME --name $RESOURCE_GROUP

# Create ACR
az acr create --resource-group $RESOURCE_GROUP --name $ACR_NAME --location $REGION_NAME --sku Standard
</code></pre> <p>List the results:</p> <pre><code>az acr list -o table
NAME      RESOURCE GROUP   LOCATION     SKU      LOGIN SERVER          CREATION DATE         ADMIN ENABLED
--------  ---------------  -----------  -------  -------------------   --------------------  ---------------
acr11044  aksResilience    chinaeast2   Standard acr11044.azurecr.cn   2021-02-05T03:12:36Z  False
</code></pre> <p>Create AKS cluster:</p> <pre><code class="language-bash">az aks create \
--resource-group $RESOURCE_GROUP \
--name $AKS_CLUSTER_NAME \
--location $REGION_NAME \
--node-count 2 \
--enable-addons monitoring \
--generate-ssh-keys
</code></pre> <p>This step is slow, please wait patiently until there are return results.</p><p>List AKS clusters:</p> <pre><code>az aks list -o table
Name           Location     ResourceGroup   KubernetesVersion   ProvisioningState   Fqdn
-------------  -----------  --------------  ------------------  ------------------  ----
aksResilience  chinanorth2  aksResilience   1.18.14             Succeeded
</code></pre> <p>Bind ACR to AKS cluster so images can be automatically authenticated when pulling from ACR to AKS cluster:</p> <pre><code class="language-bash">az aks update \
--name $AKS_CLUSTER_NAME \
--resource-group $RESOURCE_GROUP \
--attach-acr $ACR_NAME
</code></pre> <p>This step is slow, please wait patiently until there are return results.</p><p>At this point, Azure resources have been created: 1 AKS cluster total, including 1 virtual machine scale set with 2 virtual machines.</p><p>List the virtual machine scale sets behind the AKS cluster. Note that the virtual machine scale set is automatically created by Azure in another resource group:</p> <pre><code>az vmss list -o table
Name                         ResourceGroup                              Location     Zones  Capacity  Overprovision  UpgradePolicy
---------------------------  ------------------------------------------ -----------  -----  --------  -------------  ---------------
aks-nodepool1-40474697-vmss  MC_AKSRESILIENCE_AKSRESILIENCE_CHINANORTH2 chinanorth2         2         False          Manual
</code></pre> <p>Finally list the instances in the virtual machine scale set:</p> <pre><code class="language-bash">az vmss list-instances \
-g MC_AKSRESILIENCE_AKSRESILIENCE_CHINANORTH2 \
-n aks-nodepool1-40474697-vmss \
-o table \
--query &quot;[].{instanceId:instanceId, Name:name, State:provisioningState}&quot;

InstanceId    Name                             State
------------  -------------------------------  ---------
0             aks-nodepool1-40474697-vmss_0    Succeeded
1             aks-nodepool1-40474697-vmss_1    Succeeded
</code></pre> <h3>Deploy Kubernetes</h3><h4>Build image and push to image registry</h4> <pre><code class="language-bash">git clone https://github.com/xfsnow/pipelines-javascript-docker/
</code></pre> <p>Pull the source code.</p><p>First build and push the image:</p> <pre><code class="language-bash">cd app
az acr build \
--resource-group $RESOURCE_GROUP \
--registry $ACR_NAME \
--image helloworld:1.0 .
</code></pre> <p>Then get the specific URL of the image registry:</p> <pre><code class="language-bash">az acr repository show -n $ACR_NAME --repository helloworld --query &quot;registry&quot;
&quot;acr11044.azurecr.cn&quot;
</code></pre> <p>Note down this URL like acr11044.azurecr.cn. Adding helloworld:1.0 after it gives the complete image pull address, like:</p><p>acr11044.azurecr.cn/helloworld:1.0</p> <pre><code class="language-bash">cd manifests
</code></pre> <p>Find this line in hello-deployment.yml:</p> <pre><code class="language-yaml"># Container image uses ACR service on Azure
image: snowpeak.azurecr.cn/helloworld:1.3
</code></pre> <p>Replace it with your own ACR image URL "acr11044.azurecr.cn/helloworld:1.0".</p><h4>Connect to AKS cluster and deploy image</h4><p>Connect to AKS cluster:</p> <pre><code class="language-bash">az aks get-credentials --resource-group $RESOURCE_GROUP --name $AKS_CLUSTER_NAME
</code></pre> <p>First view the nodes in the AKS cluster:</p> <pre><code>kubectl get nodes -o wide
NAME                                STATUS   ROLES   AGE    VERSION    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
aks-nodepool1-40474697-vmss000000   Ready    agent   142m   v1.18.14   10.240.0.4    &lt;none&gt;        Ubuntu 18.04.5 LTS   5.4.0-1035-azure   docker://19.3.14
aks-nodepool1-40474697-vmss000001   Ready    agent   142m   v1.18.14   10.240.0.5    &lt;none&gt;        Ubuntu 18.04.5 LTS   5.4.0-1035-azure   docker://19.3.14
</code></pre> <p>Then view the pods:</p> <pre><code>kubectl get pods -o wide
No resources found in default namespace.
</code></pre> <p>No images have been deployed yet, so there are no resources.</p><p>Then use <code>cd ../manifests/</code> to go to the Kubernetes configuration file directory.</p><p>Deploy to AKS cluster with <code>kubectl apply -f hello-deployment.yml</code> and <code>kubectl apply -f hello-service.yml</code>. The initial deployed cluster looks like this:</p> <pre><code>kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES
helloworld-869d58f588-5tmpf   1/1     Running   0          71s   10.244.1.6   aks-nodepool1-40474697-vmss000001   &lt;none&gt;           &lt;none&gt;
helloworld-869d58f588-bt54r   1/1     Running   0          71s   10.244.1.7   aks-nodepool1-40474697-vmss000001   &lt;none&gt;           &lt;none&gt;
helloworld-869d58f588-qh6hn   1/1     Running   0          71s   10.244.0.6   aks-nodepool1-40474697-vmss000000   &lt;none&gt;           &lt;none&gt;
helloworld-869d58f588-w8xbx   1/1     Running   0          71s   10.244.0.7   aks-nodepool1-40474697-vmss000000   &lt;none&gt;           &lt;none&gt;
</code></pre> <p>These are the 4 running pods, 2 pods on each node.</p> <pre><code>kubectl get services
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE
helloworld   LoadBalancer   10.0.116.133    139.217.112.135   80:30754/TCP   7d17h
</code></pre> <p>This is the deployed LoadBalancer. The external IP 139.217.112.135 can be accessed directly, returning results like:</p> <pre><code class="language-bash">curl http://139.217.112.135/;
Hello world! Now is 2021-02-04 08:26:52.924.
Running on helloworld-745c979464-q54vd.
</code></pre> <p>Here "Running on helloworld-745c979464-q54vd." is exactly the name of one of the pods above.</p><p>If you make continuous requests multiple times, you'll see requests distributed roughly evenly across the 4 backend pods:</p> <pre><code class="language-bash">while(true); do curl --connect-timeout 2 http://139.217.112.135/; echo -e '\n'; sleep 0.5; done

Hello world! Now is 2021-02-04 08:34:19.523.
Running on helloworld-745c979464-q54vd.

Hello world! Now is 2021-02-04 08:34:20.160.
Running on helloworld-745c979464-ljrcv.

Hello world! Now is 2021-02-04 08:34:20.798.
Running on helloworld-745c979464-97v26.

Hello world! Now is 2021-02-04 08:34:21.427.
Running on helloworld-745c979464-q54vd.

Hello world! Now is 2021-02-04 08:34:22.56.
Running on helloworld-745c979464-l2jpm.

Hello world! Now is 2021-02-04 08:34:22.697.
Running on helloworld-745c979464-ljrcv.

Hello world! Now is 2021-02-04 08:34:23.368.
Running on helloworld-745c979464-l2jpm.

Hello world! Now is 2021-02-04 08:34:24.6.
Running on helloworld-745c979464-ljrcv.

Hello world! Now is 2021-02-04 08:34:24.659.
Running on helloworld-745c979464-l2jpm.
</code></pre> <h2>Simulate Failure</h2><p>We manually stop a virtual machine to simulate a virtual machine failure scenario.</p><ol><li>First, open real-time observation of pod running status:</li></ol> <pre><code class="language-bash">kubectl get po -o wide -w
</code></pre> <ol><li>Open a new window and run:</li></ol> <pre><code class="language-bash">while(true); do curl --connect-timeout 2 http://139.217.112.135/; echo -e '\n'; sleep 0.5; done
</code></pre> <p>To observe application access.</p><ol><li>Open another window and execute the command to stop a virtual machine:</li></ol> <pre><code class="language-bash">az vmss stop -g MC_DEMO_HELLOWORLD_CHINANORTH2 -n aks-agentpool-11798053-vmss --instance-id 0
</code></pre> <ol><li>Return to the first window to view pod running status, you'll see output similar to:</li></ol> <pre><code>kubectl get po -o wide -w
NAME                          READY   STATUS        RESTARTS   AGE     IP             NODE                                NOMINATED NODE   READINESS GATES
helloworld-9bbdbf45b-2hnz4    1/1     Running       0          119s    10.240.0.152   aks-agentpool-11798053-vmss000001   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-64qt4    1/1     Terminating   0          6m41s   10.240.0.106   aks-agentpool-11798053-vmss000000   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-gczpn    1/1     Running       0          6m37s   10.240.0.135   aks-agentpool-11798053-vmss000001   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-l6dxm    1/1     Terminating   0          5m11s   10.240.0.68    aks-agentpool-11798053-vmss000000   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-rhn2m    1/1     Terminating   0          4m45s   10.240.0.42    aks-agentpool-11798053-vmss000000   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-smtft    1/1     Running       0          118s    10.240.0.185   aks-agentpool-11798053-vmss000001   &lt;none&gt;           &lt;none&gt;
helloworld-9bbdbf45b-tmdkr    1/1     Running       0          119s    10.240.0.149   aks-agentpool-11798053-vmss000001   &lt;none&gt;           &lt;none&gt;
</code></pre> <p>Pods on the first node show they're stopping, while new pods are running on the second node.</p><ol><li>Go to the curl running window to check:</li></ol> <pre><code class="language-bash">while(true); do curl --connect-timeout 2 http://139.217.112.135/; echo -e '\n'; sleep 0.5; done

curl: (28) Connection timed out after 2013 milliseconds
...
curl: (28) Connection timed out after 2009 milliseconds
...
Hello world! Now is 2021-02-04 01:40:37.752.
Running on helloworld-9bbdbf45b-2vjxt.

Hello world! Now is 2021-02-04 01:40:38.409.
Running on helloworld-9bbdbf45b-vpvn6.

Hello world! Now is 2021-02-04 01:40:39.57.
Running on helloworld-9bbdbf45b-2vjxt.

Hello world! Now is 2021-02-04 01:40:39.736.
</code></pre> <p>After several timeouts, the return results become normal again, and new pod names appear in the hostname.</p><p>Looking carefully at the time in the output content, you can see the timeout duration doesn't exceed 1 minute.</p><ol><li>Finally, we restore node operation:</li></ol><p>You can see that pods on the originally stopped node show normal Terminating and eventually disappear. All 4 existing pods are now running on the second node. You can rebalance pod placement by setting replicas.</p><h2>Resource Cleanup</h2> <pre><code class="language-bash">az group delete --name $RESOURCE_GROUP --yes
</code></pre> <p>Delete the resource group and all its resources. The experiment is complete.</p></main><footer class="footer"><div class="container"><div class="footer-content"><div class="footer-section"><h3><i class="fas fa-snowflake"></i> Snowpeak</h3><p>Sharing and discussing cutting-edge technologies of AI and cloud computing, and other interesting topics.</p><div class="social-links"><div class="social-platforms"><a href="https://github.com/xfsnow" target="_blank" class="social-link"><i class="fab fa-github"></i></a><a href="https://snowpeak.blog.csdn.net/" target="_blank" class="social-link"><i class="fas fa-blog"></i></a><a href="https://space.bilibili.com/701839928" target="_blank" class="social-link"><i class="fab fa-bilibili"></i></a><a href="https://www.linkedin.com/in/snowpeak" target="_blank" class="social-link"><i class="fab fa-linkedin"></i></a></div><div class="wechat-section"><div class="wechat-info"><span class="wechat-label">WeChat Account</span><span class="wechat-name">Tech Warm Life</span></div><img src="/assets/img/techwarm.jpg" alt="Tech Warm Life" class="wechat-qr"></div></div></div><div class="footer-section"><h4>Quick Links</h4><ul><li><a href="/#articles">Article</a></li><li><a href="/#tools">Tools</a></li><li><a href="https://docs.github.com/en/pages" target="_blank">GitHub Pages</a></li></ul></div><div class="footer-section"><h4>Tech Stack</h4><ul><li>Python</li><li>GitHub Copilot</li><li>Claude Sonnet 4</li><li>HTML/CSS/JS</li><li>Responsive Design</li></ul></div></div><div class="footer-bottom"><p>Copyright &copy; 2013-<script>document.write((new Date()).getFullYear());</script> Snowpeak Blog. Running on GitHub Pages. <a href="https://beian.miit.gov.cn" target="_blank" class="icp_beian">京ICP备2021007720号</a><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11010502052523" target="_blank" class="gongan_beian">京公网安备11010502052523号</a></p></div></div></footer><script src="/en/index.js"></script><script src="/assets/js/blog.min.js"></script><script>hljs.highlightAll();</script></body></html>